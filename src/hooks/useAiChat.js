import { invoke } from '@tauri-apps/api/core';
import { warn } from '@tauri-apps/plugin-log';
import { formatObjectString } from '../utils/function'

const useAiChat = () => {
    const sendRequest = (url, method, headers, body, proxy) => {
        return new Promise((resolve, reject) => {

            proxy = proxy ? proxy : null;
            invoke('send_api_request', {
                request: {
                    url,
                    method,
                    headers,
                    body,
                    proxy,
                },
            }).then((response)=>{
                if (response.code == 200) {
                    resolve(response);
                } else {
                    warn("AIæ¥å£è¿”å›æ•°æ®ä¸æ­£ç¡®: " + response.msg || 'æœªçŸ¥é”™è¯¯');
                    reject(response.msg || 'æœªçŸ¥é”™è¯¯');
                }
            }).catch((error)=>{
                const infoError = formatObjectString("AIæ¥å£è¿”å›é”™è¯¯", error);
                warn(infoError);
                reject(infoError);
            });
        })
    };

    /**
     * ChatGLM æ¨¡å‹
     * @param {string} prompt ç”¨æˆ·è¾“å…¥
     * @param {string} model æ¨¡å‹åç§° (å¦‚ "glm-4", "glm-4-plus")
     * @param {string} apiKey API å¯†é’¥
     * @param {string} [proxy] å¯é€‰çš„ä»£ç†åœ°å€ (å¦‚ "http://user:pass@host:port" æˆ– "socks5://user:pass@host:port")
     * @returns {Promise<string>} AI æ¨¡å‹è¾“å‡ºæˆ–é”™è¯¯ä¿¡æ¯
     */
    const chatGLM = (prompt, model, apiKey, proxy) => {
        return new Promise((resolve, reject) => {
            const url = 'https://open.bigmodel.cn/api/paas/v4/chat/completions';
            const headers = {
                'Authorization': `Bearer ${apiKey}`,
                'Content-Type': 'application/json',
            };
            const body = {
                model: model,
                messages: [
                    {
                        role: 'user',
                        content: prompt,
                    },
                ],
                stream: false, // ç¦ç”¨æµå¼ä¼ è¾“
            };

            sendRequest(url, 'POST', headers, body, proxy).then((result)=>{
                const data = result.data.data;

                const choices = data?.choices;
                if (choices && choices.length > 0) {
                    const message = choices[0].message;
                    if (message.content) {
                        resolve(message.content);
                        return;
                    }
                }

                reject('ChatGLM å“åº”æ ¼å¼ä¸æ­£ç¡®æˆ–æ— å†…å®¹');
            }).catch((error)=>{
                // æ¥å£é”™è¯¯è¿”å›ç¤ºä¾‹ï¼š"API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : 401 Unauthorizedï¼Œå“åº”: {\"error\":{\"code\":\"401\",\"message\":\"ä»¤ç‰Œå·²è¿‡æœŸæˆ–éªŒè¯ä¸æ­£ç¡®ï¼\"}}"
                // æ‡’å¾—è§£æäº†ï¼Œç›´æ¥å±•ç¤ºç»™ç”¨æˆ·å§ğŸ˜€
                reject(error);
            });
        })
        
    };

    /**
     * DeepSeek æ¨¡å‹ *ä½œè€…æ²¡æœ‰deepSeek Api è¯¥å‡½æ•°æœªç»æµ‹è¯•ï¼*
     * @param {string} prompt ç”¨æˆ·è¾“å…¥
     * @param {string} model æ¨¡å‹åç§° (å¦‚ "deepseek-chat", "deepseek-reasoner")
     * @param {string} apiKey API å¯†é’¥
     * @param {string} [proxy] å¯é€‰çš„ä»£ç†åœ°å€
     * @returns {Promise<string>} AI æ¨¡å‹è¾“å‡ºæˆ–é”™è¯¯ä¿¡æ¯
     */
    const deepSeek = (prompt, model, apiKey, proxy) => {
        return new Promise((resolve, reject) => {
            const url = 'https://api.deepseek.com/chat/completions';
            const headers = {
                'Content-Type': 'application/json',
                'Accept': 'application/json',
                'Authorization': `Bearer ${apiKey}`,
            };
            const body = {
                messages: [
                    {
                        content: 'You are a helpful assistant',
                        role: 'system',
                    },
                    {
                        content: prompt,
                        role: 'user',
                    },
                ],
                model: model,
                stream: false,
            };

            sendRequest(url, 'POST', headers, body, proxy).then((result)=>{
                const data = result.data.data;
                const choices = data?.choices;
                if (choices && choices.length > 0 && choices[0].message && choices[0].message.content) {
                    resolve(choices[0].message.content);
                }else{
                    reject('DeepSeek å“åº”æ ¼å¼ä¸æ­£ç¡®æˆ–æ— å†…å®¹');
                }
            }).catch((error)=>{
                reject(error);
            })
        })
    };

    /**
     * Groq æ¨¡å‹
     * @param {string} prompt ç”¨æˆ·è¾“å…¥
     * @param {string} model æ¨¡å‹åç§° (å¦‚ "llama3-8b-8192", "llama3-70b-8192")
     * @param {string} apiKey API å¯†é’¥
     * @param {string} [proxy] å¯é€‰çš„ä»£ç†åœ°å€
     * @returns {Promise<string>} AI æ¨¡å‹è¾“å‡ºæˆ–é”™è¯¯ä¿¡æ¯
     */
    const groq = (prompt, model, apiKey, proxy) => {
        return new Promise((resolve, reject) => {
            const url = 'https://api.groq.com/openai/v1/chat/completions';
            const headers = {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${apiKey}`,
            };
            const body = {
                model: model,
                messages: [
                    {
                        role: 'user',
                        content: prompt,
                    },
                ],
                stream: false,
            };

            sendRequest(url, 'POST', headers, body, proxy).then((result)=>{
                const data = result.data.data;
                const choices = data?.choices;
                if (choices && choices.length > 0 && choices[0].message && choices[0].message.content) {
                    resolve(choices[0].message.content);
                }else{
                    reject('Groq å“åº”æ ¼å¼ä¸æ­£ç¡®æˆ–æ— å†…å®¹');
                }
            }).catch((error)=>{
                // æ¥å£é”™è¯¯ç¤ºä¾‹ï¼š"API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : 401 Unauthorizedï¼Œå“åº”: {"error":{"message":"Invalid API Key","type":"invalid_request_error","code":"invalid_api_key"}}"
                reject(error);
            });
        })
    };

    /**
     * Google Gemini æ¨¡å‹
     * @param {string} prompt ç”¨æˆ·è¾“å…¥
     * @param {string} model æ¨¡å‹åç§° (å¦‚ "gemini-1.5-flash", "gemini-1.5-pro")
     * @param {string} apiKey API å¯†é’¥
     * @param {string} [proxy] å¯é€‰çš„ä»£ç†åœ°å€
     * @returns {Promise<string>} AI æ¨¡å‹è¾“å‡ºæˆ–é”™è¯¯ä¿¡æ¯
     */
    const google = (prompt, model, apiKey, proxy) => {
        return new Promise((resolve, reject) => {
            const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`;
            const headers = {
                'Content-Type': 'application/json',
            };
            const body = {
                contents: [
                    {
                        parts: [
                            {
                                text: prompt,
                            },
                        ],
                    },
                ],
            };

            sendRequest(url, 'POST', headers, body, proxy).then((result)=>{
                const data = result.data.data;
                const candidates = data?.candidates;
                if (candidates && candidates.length > 0 && candidates[0].content && candidates[0].content.parts && candidates[0].content.parts.length > 0) {
                    const textPart = candidates[0].content.parts.find(part => part.text);
                    if (textPart) {
                        resolve(textPart.text);
                        return;
                    }
                }

                reject('Google Gemini å“åº”æ ¼å¼ä¸æ­£ç¡®æˆ–æ— å†…å®¹');
            }).catch((error)=>{
                /**
                 * æ¥å£é”™è¯¯ç¤ºä¾‹ï¼š
                 * "API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : 400 Bad Requestï¼Œå“åº”: {
                    "error": {
                        "code": 400,
                        "message": "API key not valid. Please pass a valid API key.",
                        "status": "INVALID_ARGUMENT",
                        "details": [
                        {
                            "@type": "type.googleapis.com/google.rpc.ErrorInfo",
                            "reason": "API_KEY_INVALID",
                            "domain": "googleapis.com",
                            "metadata": {
                            "service": "generativelanguage.googleapis.com"
                            }
                        },
                        {
                            "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
                            "locale": "en-US",
                            "message": "API key not valid. Please pass a valid API key."
                        }
                        ]
                    }
                    }
                    "
                 */
                reject(error);
            })
        })
    };

    /**
     * ChatGPT æ¨¡å‹
     * @param {string} prompt ç”¨æˆ·è¾“å…¥
     * @param {string} model æ¨¡å‹åç§° (å¦‚ "gpt-4.1", "gpt-3.5-turbo-0125")
     * @param {string} apiKey API å¯†é’¥
     * @param {string} [proxy] å¯é€‰çš„ä»£ç†åœ°å€
     * @returns {Promise<string>} AI æ¨¡å‹è¾“å‡ºæˆ–é”™è¯¯ä¿¡æ¯
     */
    const chatGPT = (prompt, model, apiKey, proxy) => {
        return new Promise((resolve, reject) => {
            const url = 'https://api.openai.com/v1/chat/completions';
            const headers = {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${apiKey}`,
            };
            const body = {
                model: model,
                messages: [
                    {
                        role: "user",
                        content: prompt
                    }
                ]
            };

            sendRequest(url, 'POST', headers, body, proxy).then((result)=>{
                const data = result.data.data;
                const choices = data?.choices;
                if (choices && choices.length > 0 && choices[0].message && choices[0].message.content) {
                    resolve(choices[0].message.content);
                }else{
                    reject('ChatGPT å“åº”æ ¼å¼ä¸æ­£ç¡®æˆ–æ— å†…å®¹');
                }
            }).catch((error)=>{
                /**
                 * æ¥å£é”™è¯¯è¿”å›ç¤ºä¾‹ï¼š"API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : 401 Unauthorizedï¼Œå“åº”: {
                        "error": {
                            "message": "Incorrect API key provided: qqqqqqqq**********1231. You can find your API key at https://platform.openai.com/account/api-keys.",
                            "type": "invalid_request_error",
                            "param": null,
                            "code": "invalid_api_key"
                        }
                    }
                    "
                 */
                reject(error);
            })
        })
    };

    /**
     * ä½¿ç”¨AIèŠå¤©ï¼Œéæµæ¨¡å¼
     * @param {String} nowAiPlatform AIå¹³å°
     * @param {String} prompt å‘é€ç»™AIçš„æ•°æ®
     * @param {String} model æ‰€ç”¨æ¨¡å‹
     * @param {String} apiKey APIå¯†é’¥
     * @param {String} proxy ä»£ç†
     * @returns {Promise<String>} AIè¿”å›çš„å†…å®¹ æˆ– é”™è¯¯ä¿¡æ¯
     */
    const aiUse = (nowAiPlatform, prompt, model, apiKey, proxy)=>{
        switch(nowAiPlatform){
            case 'ChatGLM':
                return chatGLM(prompt, model, apiKey, proxy);
            case 'DeepSeek':
                return deepSeek(prompt, model, apiKey, proxy);
            case 'Groq':
                return groq(prompt, model, apiKey, proxy);
            case 'Google':
                return google(prompt, model, apiKey, proxy);
            case 'ChatGPT':
                return chatGPT(prompt, model, apiKey, proxy);
            default :
                return Promise.reject("æœªçŸ¥çš„AIå¹³å°");
        }
    }

    /**
     * è·å–AIæ¨¡å‹åˆ—è¡¨
     * @param {String} apiKey apiå¯†é’¥
     * @param {String} proxy ä»£ç†åœ°å€
     * @param {Array} headers è¯·æ±‚å¤´ <TOKEN> ä¼šè¢«æ›¿æ¢ä¸ºå¯†é’¥
     * @param {String} url è¯·æ±‚åœ°å€
     * @param {String} method è¯·æ±‚æ–¹å¼
     * @returns {Promise} è¯·æ±‚å®Œæˆæˆ–å¤±è´¥çš„ä¿¡æ¯
     */
    const getModelList = (apiKey, proxy, headers, url, method = 'GET') => {
        const header = {};
        headers.forEach(item => {
            const array = item.split(":");
            if(array.length == 2){
                array[1] = array[1].replace("<TOKEN>", apiKey);
                header[array[0]] = array[1];
            }
        });

        // è¯·æ±‚å¤´ç»„è£…å®Œæˆ
        return sendRequest(url.replace("<TOKEN>", apiKey), method, header, null, proxy);
    }

    return {
        getModelList, aiUse
    };
};

export default useAiChat;